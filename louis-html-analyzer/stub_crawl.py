from bs4 import BeautifulSoup as BSimport urllib2#use stub_databasefrom stub_database import *from selenium import webdriverimport webbrowserfrom threading import Threadfrom Queue import *class Crawler(object):    phantomJSpath = 'C:\phantomjs-1.9.7-windows\phantomjs.exe'    wm = "http://web.archive.org"    wmstart = "http://web.archive.org/web/"    def __init__ (self, itemID):        global q        q = Queue()        self.itemID = itemID        self.db = database()        self.db.deleteSnapshots()        url = self.db.getWebsiteHomepage(itemID)        if not url.startswith("http"):            self.url = "http://" + url        else:            self.url = url                    for i in range(19):            t = Thread(target = self.__getSnapshotLinks)            t.daemon = True            t.start()                    self.url_list = []        for year in range(2014, 1995, -1):            q.put(year)        q.join()                self.url_list = sorted(set(self.url_list), reverse=True)        #print self.url_list            def __getSnapshotLinks (self):        year = q.get()        req = urllib2.Request(self.wmstart + str(year) + "0600000000*/" + self.url)        try:            page = urllib2.urlopen(req)            soup = BS(page.read())            links = soup.findAll("a")            for link in links:                if re.match("(.*)%s(.*)" % year, str(link), re.I):                    if not "*" in str(link):                        linkwm = self.wm + link["href"]                        #### list.append() is thread-safe ####                        self.url_list.append(linkwm)                        #print linkwm            q.task_done()        except urllib2.URLError, e:            q.task_done()            print e            def __getDataFromPhantomBrowser(self, url):        driver = webdriver.PhantomJS(executable_path=self.phantomJSpath)        driver.get(url)        data = driver.page_source        driver.quit()        return data        def crawlAll(self):        for i in range(80):            t = Thread(target = self.__crawlOne)            t.daemon = True            t.start()                for index, link in enumerate(self.url_list):            if self.db.isSnapshotInDB(index) == False:                q.put((index,link))        q.join()    def __crawlOne(self):        (index, link) = q.get()        date = link[27:35]        #print date, self.itemID, link        try:            data = self.__getDataFromPhantomBrowser(link)            self.db.storeSnapshot(index, date, link, data)            q.task_done()        except Exception as e:            print e            q.task_done()                def crawl(self, index_list): #list of indexes of url_list[]        for i in range(80):            t = Thread(target = self.__crawlOne)            t.daemon = True            t.start()                    for index in index_list:            if (index < 0 or index >= len(self.url_list)):                continue            if self.db.isSnapshotInDB(index) == False:                q.put((index, self.url_list[index]))        q.join()                def getNumberOfSnapshots(self):        return len(self.url_list)db = database()itemID_list = db.getItemID(2262,2266)for (itemID,) in itemID_list:    print itemID    crawler = Crawler(itemID)    print "done collecting urls"    print crawler.getNumberOfSnapshots()    #crawler.crawl((0,2,4,6,1000))    crawler.crawlAll()    print "done crawling"