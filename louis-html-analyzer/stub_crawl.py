from bs4 import BeautifulSoup as BSimport urllib2#use stub_databasefrom stub_database import *from selenium import webdriverimport webbrowserfrom threading import Threadfrom Queue import *q = Noneclass Crawler(object):    phantomJSpath = 'C:\phantomjs-1.9.7-windows\phantomjs.exe'    wm = "http://web.archive.org"    wmstart = "http://web.archive.org/web/"    def __init__ (self, itemID):        global q        q = Queue()        self.itemID = itemID        database().deleteSnapshots()        url = database().getWebsiteHomepage(itemID)        if not url.startswith("http"):            self.url = "http://" + url        else:            self.url = url                    for i in range(19):            t = Thread(target = self.__getSnapshotLinks)            t.daemon = True            t.start()                    self.url_list = []        for year in range(2014, 1995, -1):            q.put(year)        q.join()                self.url_list = sorted(set(self.url_list), reverse=True)        #print self.url_list            def __getSnapshotLinks (self):        year = q.get()        req = urllib2.Request(self.wmstart + str(year) + "0600000000*/" + self.url)        try:            page = urllib2.urlopen(req)            soup = BS(page.read())            links = soup.findAll("a")            for link in links:                if re.match("(.*)%s(.*)" % year, str(link), re.I):                    if not "*" in str(link):                        linkwm = self.wm + link["href"]                        #### list.append() is thread-safe ####                        self.url_list.append(linkwm)                        #print linkwm            q.task_done()        except urllib2.URLError, e:            q.task_done()            print e            def __getDataFromPhantomBrowser(self, url):        driver = webdriver.PhantomJS(executable_path=self.phantomJSpath)        driver.get(url)        data = driver.page_source        driver.quit()        return data        def crawlAll(self):        for i in range(40):            t = Thread(target = self.__crawlOne)            t.daemon = True            t.start()                    for index, link in enumerate(self.url_list):            if database().isSnapshotInDB(index) == False:                q.put((index,link))        q.join()    def __crawlOne(self):        while True:            (index, link) = q.get()            date = link[27:35]            #print date, self.itemID, link            try:                data = self.__getDataFromPhantomBrowser(link)                    database().storeSnapshot(index, date, link, data)                q.task_done()            except Exception as e:                print e                q.task_done()                def crawl(self, index_list): #list of indexes of url_list[]        for i in range(40):            t = Thread(target = self.__crawlOne)            t.daemon = True            t.start()        for index in index_list:            if (index < 0 or index >= len(self.url_list)):                continue                        if database().isSnapshotInDB(index) == False:                q.put((index, self.url_list[index]))        q.join()                def getNumberOfSnapshots(self):        return len(self.url_list)'''itemID_list = database().getItemID(2263,2266)for (itemID,) in itemID_list:    print itemID    crawler = Crawler(itemID)    print "done collecting urls"    print crawler.getNumberOfSnapshots()    crawler.crawlAll()    #crawler.crawlAll()    print "done crawling"'''